def evaluate_submission(labels,ranks,k=10):
"""
Script to compute the NDCG and ERR scores of a submission.
Labels is a list of lines containing the relevance labels (one line per query).
Ranks is a list of lines with the predicted ranks (again one line per query). 
The first integer in a line is the rank --in the predicted ranking--  of the first document, where first refers to the order of the data file.
k is the truncation level for NDCG
It returns the mean ERR and mean NDCG
"""

nq = len(labels) # Number of queries
assert len(ranks)==nq, 'Expected %d lines, but got %d.'%(nq,len(ranks))

err = 0.0
ndcg = 0.0

for i in range(nq):

  l = [int(x) for x in labels[i].split()]
  try:
    r = [int(x) for x in ranks[i].split()]
  except ValueError:
    raise ValueError('Non integer value on line %d'%(i+1))

  nd = len(l) # Number of documents
  assert len(r)==nd, 'Expected %d ranks at line %d, but got %d.'%(nd,i+1,len(r))

  gains = [-1]*nd # The first element is the gain of the first document in the predicted ranking
  assert max(r)<=nd, 'Ranks on line %d larger than number of documents (%d).'%(i+1,nd)
  for j in range(nd):
    gains[r[j]-1] = (2**l[j]-1.0)/16
  assert min(gains)>=0, 'Not all ranks present at line %d.'%(i+1)

  p = 1.0
  for j in range(nd):
      r = gains[j]
      err += p*r/(j+1.0)
      p *= 1-r

  dcg = sum([g/log(j+2) for (j,g) in enumerate(gains[:k])])
  gains.sort()
  gains = gains[::-1]
  ideal_dcg = sum([g/log(j+2) for (j,g) in enumerate(gains[:k])])
  if ideal_dcg:
      ndcg += dcg / ideal_dcg
  else:
      ndcg += 1.0

return (err/nq, ndcg/nq)

  
 
          
    
